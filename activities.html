<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Pritam Deka | Academic Activities</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="style.css">
</head>
<body>

  <nav>
    <a href="index.html" class="active">About</a>
    <a href="education.html">Education</a>
    <a href="papers.html">Papers</a>
    <a href="activities.html">Activities</a>
  </nav>

  <div class="main-container">
    <section>
      <h2>Academic Activities</h2>
      <ul>
        <li>Released fine-tuned language models in HuggingFace repository: <a href="https://huggingface.co/pritamdeka" target="_blank">View Models</a></li>
        <li>Created two online demos based on part of PhD work:
          <ul>
            <li><a href="https://shorturl.at/DX347" target="_blank">Demo 1</a></li>
            <li><a href="https://shorturl.at/eimuz" target="_blank">Demo 2</a></li>
          </ul>
        </li>
        <li>Demos use Python, Gradio, and state-of-the-art NLP libraries (sentence transformers, Huggingface)</li>
        <li>Reviewer for ICON 2021 (NIT Silchar, Assam, India)</li>
        <li>Speaker for webinar on "Research and Innovation on Information Technology" at Universitas Islam Negeri Walisongo, Indonesia</li>
        <li>Radio talk at University of Calicut on fake health information</li>
        <li>Main speaker for ElevateNI 2023, QUB: “The role of large language models in AI”</li>
        <li>Guest lecturer for Deep Learning module at QUB</li>
        <li>Reviewer for ACM/SIGAPP Symposium on Applied Computing 2024</li>
        <li>Program Committee member, ICON 2023</li>
        <li>Program Committee member, GPTMB 2024 (Generative Pre-trained Transformer Models and Beyond)</li>
        <li>Speaker at AICON 2024, Titanic Belfast: “AI-Powered Information Extraction”</li>
      </ul>
    </section>
  </div>
  <footer>
    <div class="container">
      &copy; 2025 Pritam Deka
    </div>
  </footer>
</body>
</html>
